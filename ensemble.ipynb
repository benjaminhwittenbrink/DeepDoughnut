{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('output_data/ucf_lat_lon_interpolated_all_chars.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [-6.125, -5.126, -4.126, -3.126, -2.126, -1.126, -0.126, 0.874, 3.874, 4.874, 5.874,\\\n",
    "6.874, 7.874, 8.874, 9.874, 11.874, 12.874, 13.874, 14.874, 15.874, 16.874, 17.875]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label']=pd.cut(df['pred_post_pct_change'], bins, labels = range(1, len(bins)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/statsmodels/tsa/tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "  x = pd.concat(x[::order], 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4431751611013474\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00        57\n",
      "           2       0.53      0.77      0.63       354\n",
      "           3       0.40      0.38      0.39       437\n",
      "           4       0.10      0.05      0.07       321\n",
      "           5       0.20      0.09      0.12       258\n",
      "           6       0.56      0.89      0.68       392\n",
      "           7       0.00      0.00      0.00        51\n",
      "           8       0.45      0.73      0.56        73\n",
      "           9       0.00      0.00      0.00        76\n",
      "          10       0.72      1.00      0.84       306\n",
      "          11       0.00      0.00      0.00        43\n",
      "          13       0.00      0.00      0.00         3\n",
      "          14       0.00      0.00      0.00        51\n",
      "          15       0.00      0.00      0.00       284\n",
      "          16       0.00      0.00      0.00       262\n",
      "          17       0.33      1.00      0.50       328\n",
      "          18       0.00      0.00      0.00        92\n",
      "          19       0.00      0.00      0.00        11\n",
      "          20       0.00      0.00      0.00         7\n",
      "          21       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.44      3414\n",
      "   macro avg       0.16      0.25      0.19      3414\n",
      "weighted avg       0.30      0.44      0.34      3414\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics \n",
    "\n",
    "X = ['pred_density2019', 'pred_wfh_emp', 'pred_dist_to_cbd', 'pred_estab_count']\n",
    "y = 'label'\n",
    "\n",
    "#scaler = StandardScaler()\n",
    "#dfx = pd.DataFrame(scaler.fit_transform(df[X]), columns = X)\n",
    "df_clean = df[X+[y]]\n",
    "df_clean = sm.add_constant(df_clean)\n",
    "df_clean=df_clean.dropna()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[X], df[y], test_size=0.33)\n",
    "\n",
    "# define the multinomial logistic regression model\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "class_report=classification_report(y_test, preds)\n",
    "print(sum(y_test==preds)/len(y_test))\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9487686692155199"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y_run, result.predict(X_run), squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in image data to evaluate on model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder \n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm \n",
    "from main import make_weights_for_balanced_classes, measure_accuracy, ImageClassificationCollator\n",
    "from models import LogisticRegression #, BasicCNNModel, DenseCNNModel\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_accuracy(outputs, labels, CLASS_NAMES):\n",
    "\tpreds = np.argmax(outputs, axis = 1).flatten()\n",
    "\tlabels = labels.flatten()\n",
    "\tcorrect = np.sum(preds == labels)\n",
    "\tbig_c_matrix = confusion_matrix(labels, preds, labels=CLASS_NAMES)\n",
    "\tc_matrices = multilabel_confusion_matrix(labels, preds, labels=CLASS_NAMES)\n",
    "\tTNs=0; FPs=0; FNs=0; TPs=0;\n",
    "\tfor c_matrix in c_matrices:\t\n",
    "\t\tTN, FP, FN, TP = c_matrix.ravel()\n",
    "\t\tTNs+=TN; FPs+=FP; FNs+=FN; TPs+=TP\n",
    "\tcorrect = np.sum(labels==preds)\n",
    "\treturn TNs, FPs, FNs, TPs, correct, len(labels), big_c_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES=20\n",
    "CLASS_NAMES = [i for i in range(CLASSES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): PatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## read in logistic_regression1.pt\n",
    "model = ViTForImageClassification.from_pretrained('models/vit/')\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader, model, criterion, device, name=\"test\", write_file=None):\n",
    "\t#need criterion, device\n",
    "\tmodel.eval()\n",
    "\ttotal_loss = 0. \n",
    "\n",
    "\ttotal_TN=0; total_FP=0; total_FN=0; total_TP=0\n",
    "\ttotal_correct=0\n",
    "\ttotal_sample = 0 \n",
    "\ttotal_confusion = np.zeros((CLASSES, CLASSES))\n",
    "\n",
    "\tpredicted_labels = []\n",
    "\n",
    "\tfor i, batch in enumerate(tqdm(loader)):\n",
    "\t\tinputs, labels = batch['pixel_values'], batch['labels'] \n",
    "\t\tinputs = inputs.to(device)\n",
    "\t\tlabels = labels.to(device)\n",
    "\n",
    "\t\twith torch.no_grad(): logits = model(inputs)['logits']\n",
    "\t\t\n",
    "\t\tpredicted_labels.append(logits)\n",
    "\n",
    "\t\tlogits = logits.to(device)\n",
    "\t\tloss = criterion(logits, labels)\n",
    "\t\ttotal_loss += loss.cpu().item()\n",
    "\n",
    "\t\tTN, FP, FN, TP, n_correct, sample_n, c_matrix  = measure_accuracy(logits.cpu().numpy(), labels.cpu().numpy(), CLASS_NAMES)\n",
    "\t\ttotal_correct+=n_correct; total_sample+=sample_n; total_confusion+=c_matrix\n",
    "\t\t\n",
    "\t\ttotal_TN+=TN; total_FP+=FP; total_FN+=FN; total_TP+=TP\n",
    "\n",
    "\tprint(f'*** Accuracy on the {name} set: {total_correct/total_sample}')\n",
    "\tprint(f'*** Precision on the {name} set: {TP/(TP+FP)}')\n",
    "\tprint(f'*** Recall on the {name} set: {TP/(TP+FN)}')\n",
    "\t#print(f'*** Confusion matrix:\\n{total_confusion}')\n",
    "\tif write_file:\n",
    "\t\twrite_file.write(f'*** Accuracy on the {name} set: {total_correct/total_sample}\\n')\n",
    "\t\twrite_file.write(f'*** Confusion matrix:\\n{total_confusion}\\n')\n",
    "\n",
    "\ttest_acc = float(total_correct / total_sample) * 100\n",
    "\n",
    "\treturn total_loss, test_acc, predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolder(root='main_data2_east_val') #lazily loads all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, class_weights = make_weights_for_balanced_classes(dataset.imgs, len(dataset.classes))\n",
    "weights = torch.FloatTensor(weights)\n",
    "class_weights = torch.FloatTensor(class_weights)\n",
    "criterion = nn.CrossEntropyLoss(weight = class_weights).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "collator = ImageClassificationCollator(feature_extractor)\n",
    "\n",
    "test_dataset = DataLoader(dataset, batch_size=32, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:48<00:00,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Accuracy on the test set: 0.2523033309709426\n",
      "*** Precision on the test set: 0.3333333333333333\n",
      "*** Recall on the test set: 0.3333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_, _, predicted_labels = test(loader=test_dataset, model=model, criterion=criterion, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels2 = [predicted_labels[i].cpu().numpy() for i in range(len(predicted_labels))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels3 = np.array([x for y in predicted_labels2 for x in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels3 = pd.DataFrame(predicted_labels3.reshape((-1, 20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels3.columns=['label_' + str(i) for i in predicted_labels3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths, _=zip(*dataset.imgs)\n",
    "new_paths = [path.split('/')[2] for path in paths]\n",
    "indices = [int(f.split('_')[0]) for f in new_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels3['merge_col']=pd.Series(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = predicted_labels3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_0</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "      <th>label_4</th>\n",
       "      <th>label_5</th>\n",
       "      <th>label_6</th>\n",
       "      <th>label_7</th>\n",
       "      <th>label_8</th>\n",
       "      <th>label_9</th>\n",
       "      <th>...</th>\n",
       "      <th>label_11</th>\n",
       "      <th>label_12</th>\n",
       "      <th>label_13</th>\n",
       "      <th>label_14</th>\n",
       "      <th>label_15</th>\n",
       "      <th>label_16</th>\n",
       "      <th>label_17</th>\n",
       "      <th>label_18</th>\n",
       "      <th>label_19</th>\n",
       "      <th>merge_col</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.529058</td>\n",
       "      <td>-0.672272</td>\n",
       "      <td>-0.828284</td>\n",
       "      <td>-1.047476</td>\n",
       "      <td>3.657719</td>\n",
       "      <td>2.420945</td>\n",
       "      <td>2.969915</td>\n",
       "      <td>-0.778145</td>\n",
       "      <td>-1.693525</td>\n",
       "      <td>-1.175701</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031584</td>\n",
       "      <td>0.180130</td>\n",
       "      <td>1.771587</td>\n",
       "      <td>5.214890</td>\n",
       "      <td>2.803714</td>\n",
       "      <td>-0.328333</td>\n",
       "      <td>-1.008610</td>\n",
       "      <td>-0.707124</td>\n",
       "      <td>-1.374020</td>\n",
       "      <td>4408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.177726</td>\n",
       "      <td>0.171371</td>\n",
       "      <td>-0.714940</td>\n",
       "      <td>-1.661297</td>\n",
       "      <td>3.356462</td>\n",
       "      <td>2.060072</td>\n",
       "      <td>2.660975</td>\n",
       "      <td>-1.033036</td>\n",
       "      <td>-0.853733</td>\n",
       "      <td>-1.154686</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.686790</td>\n",
       "      <td>0.903336</td>\n",
       "      <td>1.941999</td>\n",
       "      <td>6.253850</td>\n",
       "      <td>1.565685</td>\n",
       "      <td>-0.524279</td>\n",
       "      <td>-0.704502</td>\n",
       "      <td>-0.715605</td>\n",
       "      <td>-0.720212</td>\n",
       "      <td>4422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.421651</td>\n",
       "      <td>0.621193</td>\n",
       "      <td>-0.875519</td>\n",
       "      <td>-1.583046</td>\n",
       "      <td>1.252312</td>\n",
       "      <td>1.149874</td>\n",
       "      <td>1.612896</td>\n",
       "      <td>-0.394665</td>\n",
       "      <td>0.054276</td>\n",
       "      <td>-0.966672</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.065666</td>\n",
       "      <td>1.338967</td>\n",
       "      <td>2.495767</td>\n",
       "      <td>7.124341</td>\n",
       "      <td>2.187498</td>\n",
       "      <td>-0.276057</td>\n",
       "      <td>-0.847643</td>\n",
       "      <td>-0.310195</td>\n",
       "      <td>-0.693611</td>\n",
       "      <td>4423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.672412</td>\n",
       "      <td>-0.482491</td>\n",
       "      <td>-0.868200</td>\n",
       "      <td>-1.024541</td>\n",
       "      <td>2.039017</td>\n",
       "      <td>0.803414</td>\n",
       "      <td>1.053237</td>\n",
       "      <td>-1.229863</td>\n",
       "      <td>-2.069312</td>\n",
       "      <td>-1.152663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.453309</td>\n",
       "      <td>0.711398</td>\n",
       "      <td>3.793168</td>\n",
       "      <td>6.514907</td>\n",
       "      <td>3.794643</td>\n",
       "      <td>-0.494839</td>\n",
       "      <td>-0.547396</td>\n",
       "      <td>-0.426675</td>\n",
       "      <td>-1.486368</td>\n",
       "      <td>4426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.430698</td>\n",
       "      <td>0.628553</td>\n",
       "      <td>-0.710090</td>\n",
       "      <td>-1.138537</td>\n",
       "      <td>0.848180</td>\n",
       "      <td>-0.055030</td>\n",
       "      <td>0.373838</td>\n",
       "      <td>-0.970629</td>\n",
       "      <td>-0.720325</td>\n",
       "      <td>-1.063880</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.681198</td>\n",
       "      <td>1.840883</td>\n",
       "      <td>2.906957</td>\n",
       "      <td>6.714512</td>\n",
       "      <td>3.419033</td>\n",
       "      <td>0.100626</td>\n",
       "      <td>0.092384</td>\n",
       "      <td>0.499582</td>\n",
       "      <td>-0.830592</td>\n",
       "      <td>6741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>-0.203982</td>\n",
       "      <td>0.243386</td>\n",
       "      <td>0.296892</td>\n",
       "      <td>-1.376281</td>\n",
       "      <td>-0.436716</td>\n",
       "      <td>-0.730272</td>\n",
       "      <td>-0.391213</td>\n",
       "      <td>-0.189076</td>\n",
       "      <td>-0.089640</td>\n",
       "      <td>-0.601886</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.894859</td>\n",
       "      <td>0.640258</td>\n",
       "      <td>-0.797403</td>\n",
       "      <td>0.212239</td>\n",
       "      <td>-0.958571</td>\n",
       "      <td>-0.962498</td>\n",
       "      <td>-0.937307</td>\n",
       "      <td>2.432998</td>\n",
       "      <td>7.194004</td>\n",
       "      <td>3497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>-0.877032</td>\n",
       "      <td>-0.414930</td>\n",
       "      <td>-0.135615</td>\n",
       "      <td>-1.288178</td>\n",
       "      <td>-0.969486</td>\n",
       "      <td>-1.360254</td>\n",
       "      <td>0.356759</td>\n",
       "      <td>0.842337</td>\n",
       "      <td>-0.540720</td>\n",
       "      <td>-0.575196</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.347059</td>\n",
       "      <td>0.261281</td>\n",
       "      <td>-0.478108</td>\n",
       "      <td>0.020949</td>\n",
       "      <td>1.239966</td>\n",
       "      <td>-0.628931</td>\n",
       "      <td>2.170654</td>\n",
       "      <td>7.449111</td>\n",
       "      <td>2.317293</td>\n",
       "      <td>3596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1408</th>\n",
       "      <td>-0.370197</td>\n",
       "      <td>-0.576587</td>\n",
       "      <td>0.524093</td>\n",
       "      <td>-1.077670</td>\n",
       "      <td>-1.387510</td>\n",
       "      <td>-1.432783</td>\n",
       "      <td>-0.635655</td>\n",
       "      <td>0.040635</td>\n",
       "      <td>-0.320703</td>\n",
       "      <td>0.393489</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.019457</td>\n",
       "      <td>-0.071306</td>\n",
       "      <td>-0.066461</td>\n",
       "      <td>-1.191395</td>\n",
       "      <td>-0.076136</td>\n",
       "      <td>-1.358922</td>\n",
       "      <td>4.308955</td>\n",
       "      <td>7.073775</td>\n",
       "      <td>1.445572</td>\n",
       "      <td>3603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1409</th>\n",
       "      <td>-1.164890</td>\n",
       "      <td>-1.006400</td>\n",
       "      <td>0.782440</td>\n",
       "      <td>0.678122</td>\n",
       "      <td>-0.773088</td>\n",
       "      <td>-0.846484</td>\n",
       "      <td>-0.975124</td>\n",
       "      <td>0.449184</td>\n",
       "      <td>-0.813567</td>\n",
       "      <td>-1.093453</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.997444</td>\n",
       "      <td>0.172792</td>\n",
       "      <td>0.436217</td>\n",
       "      <td>-0.353437</td>\n",
       "      <td>-0.739387</td>\n",
       "      <td>-1.074814</td>\n",
       "      <td>-0.806125</td>\n",
       "      <td>2.493350</td>\n",
       "      <td>7.284929</td>\n",
       "      <td>4038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1410</th>\n",
       "      <td>-0.767605</td>\n",
       "      <td>0.004514</td>\n",
       "      <td>-0.811006</td>\n",
       "      <td>-1.200324</td>\n",
       "      <td>-0.744418</td>\n",
       "      <td>-1.446086</td>\n",
       "      <td>0.723883</td>\n",
       "      <td>1.975875</td>\n",
       "      <td>-0.040012</td>\n",
       "      <td>-0.766368</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.787895</td>\n",
       "      <td>1.191072</td>\n",
       "      <td>0.977402</td>\n",
       "      <td>1.780677</td>\n",
       "      <td>2.386258</td>\n",
       "      <td>-0.332247</td>\n",
       "      <td>1.393418</td>\n",
       "      <td>6.387114</td>\n",
       "      <td>0.570418</td>\n",
       "      <td>4041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1411 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label_0   label_1   label_2   label_3   label_4   label_5   label_6  \\\n",
       "0    -0.529058 -0.672272 -0.828284 -1.047476  3.657719  2.420945  2.969915   \n",
       "1    -0.177726  0.171371 -0.714940 -1.661297  3.356462  2.060072  2.660975   \n",
       "2    -0.421651  0.621193 -0.875519 -1.583046  1.252312  1.149874  1.612896   \n",
       "3    -0.672412 -0.482491 -0.868200 -1.024541  2.039017  0.803414  1.053237   \n",
       "4    -0.430698  0.628553 -0.710090 -1.138537  0.848180 -0.055030  0.373838   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1406 -0.203982  0.243386  0.296892 -1.376281 -0.436716 -0.730272 -0.391213   \n",
       "1407 -0.877032 -0.414930 -0.135615 -1.288178 -0.969486 -1.360254  0.356759   \n",
       "1408 -0.370197 -0.576587  0.524093 -1.077670 -1.387510 -1.432783 -0.635655   \n",
       "1409 -1.164890 -1.006400  0.782440  0.678122 -0.773088 -0.846484 -0.975124   \n",
       "1410 -0.767605  0.004514 -0.811006 -1.200324 -0.744418 -1.446086  0.723883   \n",
       "\n",
       "       label_7   label_8   label_9  ...  label_11  label_12  label_13  \\\n",
       "0    -0.778145 -1.693525 -1.175701  ... -0.031584  0.180130  1.771587   \n",
       "1    -1.033036 -0.853733 -1.154686  ... -0.686790  0.903336  1.941999   \n",
       "2    -0.394665  0.054276 -0.966672  ... -1.065666  1.338967  2.495767   \n",
       "3    -1.229863 -2.069312 -1.152663  ... -0.453309  0.711398  3.793168   \n",
       "4    -0.970629 -0.720325 -1.063880  ... -0.681198  1.840883  2.906957   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1406 -0.189076 -0.089640 -0.601886  ... -0.894859  0.640258 -0.797403   \n",
       "1407  0.842337 -0.540720 -0.575196  ... -0.347059  0.261281 -0.478108   \n",
       "1408  0.040635 -0.320703  0.393489  ... -1.019457 -0.071306 -0.066461   \n",
       "1409  0.449184 -0.813567 -1.093453  ... -0.997444  0.172792  0.436217   \n",
       "1410  1.975875 -0.040012 -0.766368  ... -0.787895  1.191072  0.977402   \n",
       "\n",
       "      label_14  label_15  label_16  label_17  label_18  label_19  merge_col  \n",
       "0     5.214890  2.803714 -0.328333 -1.008610 -0.707124 -1.374020       4408  \n",
       "1     6.253850  1.565685 -0.524279 -0.704502 -0.715605 -0.720212       4422  \n",
       "2     7.124341  2.187498 -0.276057 -0.847643 -0.310195 -0.693611       4423  \n",
       "3     6.514907  3.794643 -0.494839 -0.547396 -0.426675 -1.486368       4426  \n",
       "4     6.714512  3.419033  0.100626  0.092384  0.499582 -0.830592       6741  \n",
       "...        ...       ...       ...       ...       ...       ...        ...  \n",
       "1406  0.212239 -0.958571 -0.962498 -0.937307  2.432998  7.194004       3497  \n",
       "1407  0.020949  1.239966 -0.628931  2.170654  7.449111  2.317293       3596  \n",
       "1408 -1.191395 -0.076136 -1.358922  4.308955  7.073775  1.445572       3603  \n",
       "1409 -0.353437 -0.739387 -1.074814 -0.806125  2.493350  7.284929       4038  \n",
       "1410  1.780677  2.386258 -0.332247  1.393418  6.387114  0.570418       4041  \n",
       "\n",
       "[1411 rows x 21 columns]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.merge(labels_df, on='merge_col')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>compass</th>\n",
       "      <th>merge_col</th>\n",
       "      <th>zip</th>\n",
       "      <th>post_pct_change</th>\n",
       "      <th>pre_pct_change</th>\n",
       "      <th>density2019</th>\n",
       "      <th>wfh_emp</th>\n",
       "      <th>...</th>\n",
       "      <th>label_10</th>\n",
       "      <th>label_11</th>\n",
       "      <th>label_12</th>\n",
       "      <th>label_13</th>\n",
       "      <th>label_14</th>\n",
       "      <th>label_15</th>\n",
       "      <th>label_16</th>\n",
       "      <th>label_17</th>\n",
       "      <th>label_18</th>\n",
       "      <th>label_19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>40.440271</td>\n",
       "      <td>-80.006800</td>\n",
       "      <td>119.24</td>\n",
       "      <td>2</td>\n",
       "      <td>15211</td>\n",
       "      <td>18.814366</td>\n",
       "      <td>8.243265</td>\n",
       "      <td>6657.455182</td>\n",
       "      <td>0.401119</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.746721</td>\n",
       "      <td>-0.164411</td>\n",
       "      <td>0.488373</td>\n",
       "      <td>0.565465</td>\n",
       "      <td>3.764569</td>\n",
       "      <td>0.315173</td>\n",
       "      <td>-1.478644</td>\n",
       "      <td>-1.884411</td>\n",
       "      <td>-0.059268</td>\n",
       "      <td>-0.604174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>40.440189</td>\n",
       "      <td>-80.006600</td>\n",
       "      <td>118.68</td>\n",
       "      <td>4</td>\n",
       "      <td>15211</td>\n",
       "      <td>18.814366</td>\n",
       "      <td>8.243265</td>\n",
       "      <td>6657.455182</td>\n",
       "      <td>0.401119</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.072316</td>\n",
       "      <td>-1.151779</td>\n",
       "      <td>1.567367</td>\n",
       "      <td>2.636570</td>\n",
       "      <td>6.455995</td>\n",
       "      <td>1.817873</td>\n",
       "      <td>-0.359595</td>\n",
       "      <td>-0.913241</td>\n",
       "      <td>-0.814075</td>\n",
       "      <td>-0.842576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>40.439720</td>\n",
       "      <td>-80.005300</td>\n",
       "      <td>115.87</td>\n",
       "      <td>14</td>\n",
       "      <td>15211</td>\n",
       "      <td>18.814366</td>\n",
       "      <td>8.243265</td>\n",
       "      <td>6657.455182</td>\n",
       "      <td>0.401119</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.948428</td>\n",
       "      <td>-1.077294</td>\n",
       "      <td>-0.045520</td>\n",
       "      <td>0.792635</td>\n",
       "      <td>2.127646</td>\n",
       "      <td>0.494998</td>\n",
       "      <td>-0.407948</td>\n",
       "      <td>-2.152733</td>\n",
       "      <td>-1.069296</td>\n",
       "      <td>-1.174233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>40.439475</td>\n",
       "      <td>-80.004000</td>\n",
       "      <td>114.74</td>\n",
       "      <td>18</td>\n",
       "      <td>15222</td>\n",
       "      <td>8.064086</td>\n",
       "      <td>2.621717</td>\n",
       "      <td>6583.614873</td>\n",
       "      <td>0.484400</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.844793</td>\n",
       "      <td>-1.871219</td>\n",
       "      <td>-1.168997</td>\n",
       "      <td>-1.181048</td>\n",
       "      <td>-0.943008</td>\n",
       "      <td>-1.862635</td>\n",
       "      <td>-1.224407</td>\n",
       "      <td>-2.236741</td>\n",
       "      <td>-2.171866</td>\n",
       "      <td>-1.151943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>40.439381</td>\n",
       "      <td>-80.004400</td>\n",
       "      <td>114.18</td>\n",
       "      <td>20</td>\n",
       "      <td>15222</td>\n",
       "      <td>8.064086</td>\n",
       "      <td>2.621717</td>\n",
       "      <td>6583.614873</td>\n",
       "      <td>0.484400</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.250169</td>\n",
       "      <td>-0.985914</td>\n",
       "      <td>-0.036094</td>\n",
       "      <td>0.902289</td>\n",
       "      <td>2.437139</td>\n",
       "      <td>-0.350065</td>\n",
       "      <td>-1.385200</td>\n",
       "      <td>-2.140086</td>\n",
       "      <td>-1.411850</td>\n",
       "      <td>-1.419021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>8772</td>\n",
       "      <td>40.729771</td>\n",
       "      <td>-73.986812</td>\n",
       "      <td>143.92</td>\n",
       "      <td>8773</td>\n",
       "      <td>10003</td>\n",
       "      <td>-0.508407</td>\n",
       "      <td>-3.709946</td>\n",
       "      <td>93637.009680</td>\n",
       "      <td>0.505173</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.515669</td>\n",
       "      <td>-0.691545</td>\n",
       "      <td>3.720292</td>\n",
       "      <td>4.842623</td>\n",
       "      <td>6.361959</td>\n",
       "      <td>4.914660</td>\n",
       "      <td>-0.075143</td>\n",
       "      <td>-0.564232</td>\n",
       "      <td>0.270296</td>\n",
       "      <td>-0.444728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>8776</td>\n",
       "      <td>40.729586</td>\n",
       "      <td>-73.986371</td>\n",
       "      <td>158.48</td>\n",
       "      <td>8777</td>\n",
       "      <td>10003</td>\n",
       "      <td>-0.508407</td>\n",
       "      <td>-3.709946</td>\n",
       "      <td>93637.009680</td>\n",
       "      <td>0.505173</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.390951</td>\n",
       "      <td>-0.482745</td>\n",
       "      <td>2.829035</td>\n",
       "      <td>4.687967</td>\n",
       "      <td>6.888363</td>\n",
       "      <td>4.276443</td>\n",
       "      <td>-0.086221</td>\n",
       "      <td>-0.867466</td>\n",
       "      <td>-0.481046</td>\n",
       "      <td>-0.613991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1408</th>\n",
       "      <td>8781</td>\n",
       "      <td>40.729330</td>\n",
       "      <td>-73.985757</td>\n",
       "      <td>176.68</td>\n",
       "      <td>8782</td>\n",
       "      <td>10003</td>\n",
       "      <td>-0.508407</td>\n",
       "      <td>-3.709946</td>\n",
       "      <td>93637.009680</td>\n",
       "      <td>0.505173</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.373208</td>\n",
       "      <td>-0.649045</td>\n",
       "      <td>1.636981</td>\n",
       "      <td>4.776500</td>\n",
       "      <td>6.247315</td>\n",
       "      <td>5.745951</td>\n",
       "      <td>0.135550</td>\n",
       "      <td>-0.537219</td>\n",
       "      <td>0.083764</td>\n",
       "      <td>-0.906110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1409</th>\n",
       "      <td>8784</td>\n",
       "      <td>40.729164</td>\n",
       "      <td>-73.985359</td>\n",
       "      <td>187.60</td>\n",
       "      <td>8785</td>\n",
       "      <td>10003</td>\n",
       "      <td>-0.508407</td>\n",
       "      <td>-3.709946</td>\n",
       "      <td>93637.009680</td>\n",
       "      <td>0.505173</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.312394</td>\n",
       "      <td>-0.364262</td>\n",
       "      <td>0.854075</td>\n",
       "      <td>3.117109</td>\n",
       "      <td>4.975822</td>\n",
       "      <td>6.986619</td>\n",
       "      <td>0.635137</td>\n",
       "      <td>-0.848219</td>\n",
       "      <td>-0.544822</td>\n",
       "      <td>-1.117901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1410</th>\n",
       "      <td>8790</td>\n",
       "      <td>40.730794</td>\n",
       "      <td>-73.989228</td>\n",
       "      <td>333.20</td>\n",
       "      <td>8791</td>\n",
       "      <td>10003</td>\n",
       "      <td>-0.508407</td>\n",
       "      <td>-3.709946</td>\n",
       "      <td>93637.009680</td>\n",
       "      <td>0.505173</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.709254</td>\n",
       "      <td>-0.268165</td>\n",
       "      <td>2.804006</td>\n",
       "      <td>2.596866</td>\n",
       "      <td>3.649556</td>\n",
       "      <td>6.945777</td>\n",
       "      <td>0.096186</td>\n",
       "      <td>-0.249921</td>\n",
       "      <td>1.151857</td>\n",
       "      <td>-0.664315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1411 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0        lat        lon  compass  merge_col    zip  \\\n",
       "0              1  40.440271 -80.006800   119.24          2  15211   \n",
       "1              3  40.440189 -80.006600   118.68          4  15211   \n",
       "2             13  40.439720 -80.005300   115.87         14  15211   \n",
       "3             17  40.439475 -80.004000   114.74         18  15222   \n",
       "4             19  40.439381 -80.004400   114.18         20  15222   \n",
       "...          ...        ...        ...      ...        ...    ...   \n",
       "1406        8772  40.729771 -73.986812   143.92       8773  10003   \n",
       "1407        8776  40.729586 -73.986371   158.48       8777  10003   \n",
       "1408        8781  40.729330 -73.985757   176.68       8782  10003   \n",
       "1409        8784  40.729164 -73.985359   187.60       8785  10003   \n",
       "1410        8790  40.730794 -73.989228   333.20       8791  10003   \n",
       "\n",
       "      post_pct_change  pre_pct_change   density2019   wfh_emp  ...  label_10  \\\n",
       "0           18.814366        8.243265   6657.455182  0.401119  ... -0.746721   \n",
       "1           18.814366        8.243265   6657.455182  0.401119  ... -0.072316   \n",
       "2           18.814366        8.243265   6657.455182  0.401119  ... -0.948428   \n",
       "3            8.064086        2.621717   6583.614873  0.484400  ... -0.844793   \n",
       "4            8.064086        2.621717   6583.614873  0.484400  ... -0.250169   \n",
       "...               ...             ...           ...       ...  ...       ...   \n",
       "1406        -0.508407       -3.709946  93637.009680  0.505173  ... -0.515669   \n",
       "1407        -0.508407       -3.709946  93637.009680  0.505173  ... -0.390951   \n",
       "1408        -0.508407       -3.709946  93637.009680  0.505173  ... -0.373208   \n",
       "1409        -0.508407       -3.709946  93637.009680  0.505173  ... -0.312394   \n",
       "1410        -0.508407       -3.709946  93637.009680  0.505173  ... -0.709254   \n",
       "\n",
       "      label_11  label_12  label_13  label_14  label_15  label_16  label_17  \\\n",
       "0    -0.164411  0.488373  0.565465  3.764569  0.315173 -1.478644 -1.884411   \n",
       "1    -1.151779  1.567367  2.636570  6.455995  1.817873 -0.359595 -0.913241   \n",
       "2    -1.077294 -0.045520  0.792635  2.127646  0.494998 -0.407948 -2.152733   \n",
       "3    -1.871219 -1.168997 -1.181048 -0.943008 -1.862635 -1.224407 -2.236741   \n",
       "4    -0.985914 -0.036094  0.902289  2.437139 -0.350065 -1.385200 -2.140086   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1406 -0.691545  3.720292  4.842623  6.361959  4.914660 -0.075143 -0.564232   \n",
       "1407 -0.482745  2.829035  4.687967  6.888363  4.276443 -0.086221 -0.867466   \n",
       "1408 -0.649045  1.636981  4.776500  6.247315  5.745951  0.135550 -0.537219   \n",
       "1409 -0.364262  0.854075  3.117109  4.975822  6.986619  0.635137 -0.848219   \n",
       "1410 -0.268165  2.804006  2.596866  3.649556  6.945777  0.096186 -0.249921   \n",
       "\n",
       "      label_18  label_19  \n",
       "0    -0.059268 -0.604174  \n",
       "1    -0.814075 -0.842576  \n",
       "2    -1.069296 -1.174233  \n",
       "3    -2.171866 -1.151943  \n",
       "4    -1.411850 -1.419021  \n",
       "...        ...       ...  \n",
       "1406  0.270296 -0.444728  \n",
       "1407 -0.481046 -0.613991  \n",
       "1408  0.083764 -0.906110  \n",
       "1409 -0.544822 -1.117901  \n",
       "1410  1.151857 -0.664315  \n",
       "\n",
       "[1411 rows x 40 columns]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X = ['label_' + str(i) for i in range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/statsmodels/tsa/tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "  x = pd.concat(x[::order], 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5107296137339056\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00        12\n",
      "           2       0.59      0.89      0.71        56\n",
      "           3       0.42      0.28      0.33        47\n",
      "           4       0.58      0.45      0.51        42\n",
      "           5       0.45      0.68      0.54        31\n",
      "           6       0.68      0.63      0.66        41\n",
      "           7       0.00      0.00      0.00         6\n",
      "           9       0.00      0.00      0.00        12\n",
      "          10       0.75      1.00      0.86        54\n",
      "          11       0.00      0.00      0.00         6\n",
      "          13       0.00      0.00      0.00         1\n",
      "          14       0.00      0.00      0.00         8\n",
      "          15       0.47      0.20      0.28        40\n",
      "          16       0.06      0.05      0.06        39\n",
      "          17       0.41      1.00      0.58        45\n",
      "          18       0.00      0.00      0.00        16\n",
      "          19       0.00      0.00      0.00         4\n",
      "          20       0.00      0.00      0.00         1\n",
      "          21       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.51       466\n",
      "   macro avg       0.23      0.27      0.24       466\n",
      "weighted avg       0.43      0.51      0.44       466\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics \n",
    "\n",
    "X = ['pred_density2019', 'pred_wfh_emp', 'pred_dist_to_cbd', 'pred_estab_count']+new_X\n",
    "y = 'label'\n",
    "\n",
    "#scaler = StandardScaler()\n",
    "#dfx = pd.DataFrame(scaler.fit_transform(df[X]), columns = X)\n",
    "df_clean = df[X+[y]]\n",
    "df_clean = sm.add_constant(df_clean)\n",
    "df_clean=df_clean.dropna()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[X], df[y], test_size=0.33, random_state=30)\n",
    "\n",
    "# define the multinomial logistic regression model\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "class_report=classification_report(y_test, preds)\n",
    "print(sum(y_test==preds)/len(y_test))\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5236051502145923\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00        12\n",
      "           2       0.60      0.79      0.68        56\n",
      "           3       0.41      0.38      0.40        47\n",
      "           4       0.61      0.40      0.49        42\n",
      "           5       0.43      0.68      0.53        31\n",
      "           6       0.68      0.68      0.68        41\n",
      "           7       0.00      0.00      0.00         6\n",
      "           9       0.00      0.00      0.00        12\n",
      "          10       0.75      1.00      0.86        54\n",
      "          11       0.00      0.00      0.00         6\n",
      "          13       0.00      0.00      0.00         1\n",
      "          14       0.00      0.00      0.00         8\n",
      "          15       0.47      0.20      0.28        40\n",
      "          16       0.38      0.95      0.54        39\n",
      "          17       0.38      0.38      0.38        45\n",
      "          18       0.00      0.00      0.00        16\n",
      "          19       0.00      0.00      0.00         4\n",
      "          20       0.00      0.00      0.00         1\n",
      "          21       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.52       466\n",
      "   macro avg       0.25      0.29      0.25       466\n",
      "weighted avg       0.45      0.52      0.47       466\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/statsmodels/tsa/tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "  x = pd.concat(x[::order], 1)\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics \n",
    "\n",
    "X = ['pred_density2019', 'pred_wfh_emp', 'pred_dist_to_cbd', 'pred_estab_count']\n",
    "y = 'label'\n",
    "\n",
    "#scaler = StandardScaler()\n",
    "#dfx = pd.DataFrame(scaler.fit_transform(df[X]), columns = X)\n",
    "df_clean = df[X+[y]]\n",
    "df_clean = sm.add_constant(df_clean)\n",
    "df_clean=df_clean.dropna()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[X], df[y], test_size=0.33, random_state=30)\n",
    "\n",
    "# define the multinomial logistic regression model\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "class_report=classification_report(y_test, preds)\n",
    "print(sum(y_test==preds)/len(y_test))\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "176d46c06090c506a8a8e44725eb4398ec57f0b41577ac5088b9cf5cb0238964"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('pytorch_p37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
